---
title:     "Practical tools to make data research easier and better"
subtitle:  "ESICM Tech Lounge"
author:    "@kevin_kunzmann"
institute: "MRC-BSU, University of Cambridge"
date:      "2019/09/30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

# Who am I?

* Mathematician/Statistician with keen interest in 'Data Science' and (deep) machine learning

* Worked several years as analyst in two multi-centric studies: 4C and CENTER-TBI

* Convinced that open science/source is the future

* Sometimes frustrated by the adoption of modern *tooling* in academia





---
class: inverse, middle
# Outline

1. Where are we headed? 

2. Data, Data, Data!

2. Literate Programming

3. Workflow Management

4. Version Control and Collaboration

5. Dependency Management

6. Demo





---
# Data research as software development

- modern empricial research is **highly data-driven**

- **multiple stake-holders** (analysts, non-tech experts)

- anything beyond absolute basics: **complex analysis scripts**

- **agile development** (changing requirements/hypotheses)

- quality control - **reproducibility?**





---
class: inverse, middle

> *Data research is essentially about writing a piece of software that
turns data into a manuscript. <br> <br> $\leadsto$ Be smart, adopt lessons learnt from software develoment!*



---

.pull-left[

## Reproducible Research

- **credibility/transparency**

- code **re-usability**

- easier **modification/extension**

- facilitates **passing on projects** to others

- easier in the **long run**

]

.pull-right[

## Open Science

- **sharing** results

- independent of reproducibility

- easy data access (minimal restrictions) 

- adds value to reprodicible analysis 

- funding requirement (FAIR)

]

## References

- online WiP compendium on OS & RR: [The Turing Way](https://the-turing-way.netlify.com/introduction/introduction)

- info about [FAIR](https://www.go-fair.org/fair-principles/) (Findable, Accessible, Interoperable, Reusable)


---
class: inverse, middle
# 'Data is the new oil'



---
# 'Data is the new oil' - but...

.pull-left[

<img src="figures/cash.jpg" height="200px"> 

- valuable

- basis for everything we do

]

.pull-right[

<img src="figures/deepwater.jpg" height="200px"> 

- must be handled with care 

- sticky if raw, must be processed (curated)

- requires huge infrastructure + maintainance

]




---
# Data management considerations

- have a plan and **involve domain experts, early**!

- make sure data bases are **flexible** to adapt to **changing requirements**

- think about storage of **intermediate data products**

- make sure everything is **accessible in a systematic way** 
  * REST API access?
  * permission system/privacy?

- provide metadata: units, descriptions, labels

- ensure continuous **support/maintainance over the entire project timeline**

- put more effort and money in **data curation/quality control**!

- complex data bases evolve over time, provide **versioned access** to data?





---
class: inverse, middle
# 'Literate Programming'

> *"I believe that the time is ripe for significantly better documentation of 
programs, and that we can best achieve this by considering programs to be works
of literature. Hence, my title: 'Literate Programming'."* Donald Knuth, 1984





---
# Literate Programming

- script file with code + comments: **ugly**, no output

- 'literate program': **code, output, and documentation are equally** important

- prime example 'notebooks' (**Jupyter Notebook**): 
  - **interactive** cell-style programming
  - text can be authored with **markdown**
  - can be **exported** to .pdf/.html/...




---
class: inverse, middle
# Demo: Jupyter Notebook

1. go to [**`https://bit.ly/2mAyOxn`**](https://bit.ly/2mAyOxn)

2. execute code 'chunks' (focus and shift + enter)

3. export via `File/Export notebook as`





---
# Notebooks

.pull-left[

## Pros

1. mix text/output with code

2. export to multiple formats

3. interactive coding style 

4. support most common scripting languages

]

.pull-right[

## Cons

1. plain notebook file not human-readable (json metadata)

2. interactivity: order-of-execution errors

3. limited formatting flexibility

]





---
class: inverse, middle
# markdown + knitr + pandoc




---
# Literate programming with code-chunks

- this presentation is authored in markdown (`slides.Rmd`)!

- easy to use

- source document is readable even with just a text editor

- rich output with templates

### Process:

1. combine markdown text with code in 'chunks'
2. run code chunks with appropriate scripting language (knitr)
3. capture output (text/figures) in extended markdown file (knitr)
4. convert to desired output format using pandoc (allows templates!)








---

.pull-left[

## Input markdown + code

````
Some text
`r ''````{r demo-chunk}
plot(cars)
```
````

]

.pull-right[

## Output (html)

Some text
```{r, echo=FALSE}
plot(cars)
```

]






---
class: inverse, middle
# Demo

1. go to **`https://bit.ly/2nAlE3v`**

2. open file `report.Rmd`

3. 'knit' it (button above .Rmd file)





---
class: inverse, middle
# Workflow Automation





---
# Workflows

- complex analyses often require **multiple outputs** (plots, tables, reports)

- often **intermediate files** (data sets)

- **interdependent steps**, need to maintain right **order of execution**

- difficult to update everything if something changes early on

- long running steps should not be run multiple times



### Solution: Workflow automation

- analysis = program mapping **data $\to$ outputs**

- represent **path from data to output as directed (acyclic) graph (DAG)**

- only execute necessary steps, **cache intermediates**




---
# Example: snakemake

- python tool inspired by GNU make

- python-like syntax, easy to learn

- represents workflows as DAGs

- automatically caching

- can run independent steps in parallel 

- High Performance Computing (HPC) support via slurm

- individual steps can be run in containers via singularity




---
class: inverse, middle
# Demo

1. go to [**`https://bit.ly/2lXMiTg`**](https://bit.ly/2lXMiTg)

2. have a look at the `Snakefile`

3. open a new terminal window

4. execute `snakemake --dag | dot -Tpdf > dag.pdf` and open the file `dag.pdf`

4. execute the command `snakemake` in the terminal





---
# Version Control (git)

- time machine for entire projects

- distributed (no central server)

- robust (designed for wortk on Linux kernel, now also used by Microsoft!)

- **the** standard VCS .





---
# Code hosting 

- git is a command line tool

- GitHub.com/GitLab.com are code hosting platforms that make it more productive

- 





---
# GitHub.com example repo

[demo, show features]





---
# Dependency management

- source code is fine versioned

- workflow is automated

- what could go wrong?

- lots of software dependencies (R/Python/tensorflow/snakemake/LaTeX/...)

- all changing quickly - how can we make sure that the exact computing environment can be replicated in the future?

- package managers not sufficient since cross-language/system level dependencies not captured (openssl version e.g.)




---
# Solution: Containerization

- better: isolate everything but the bare operating system (linux!)

- think: lightweight virtual machine 
- no 'hard drive'
- no hardware virtualization (use host system) - fast!
- images are single files, easily sharable!
  
- those must be complicated...





---
# Singularity container file

[example]
  
  
  
  
  
---
# Singularity vs. Docker

.pull-left[

Docker

1. made containers 'sexy'

2. widely used by industry to run microservices

3. requires root access to build and **run** $\leadsto$ incompatible with HPC systems

]

.pull-right[

Singularity

1. Relatively new (2016) 'Docker for HPC'

2. Key difference: **root only required for building** containers!

3. Geared towards HPC: reproducible, portable computing environment - run 
arbitrary software on your HPC without bothering your admin! 

4. Integrates with workflow management engines (snakemake, nextflow, etc.) and
HPC

]



---
class: inverse
# Live demo on our HPC

...




---
class: inverse
# Summary

- No free lunch: 'Data Research' is complex!

- 'Data Research' is software development, be smart and use their tools!

- Reproducibility of complex analyses paramount to enable thorough review and reuse!

- Literate programming can ease analyst/investigator communication by 
streamlining reliable report generation

- Data hosting + code hosting + containerization + version control can facilitate
collaboration on complex projects

- Better code quality $\leadsto$ more enjoyable coding + more confidence in
results
