---
title:     "Practical tools to make data research easier and better"
subtitle:  "ESICM Tech Lounge"
author:    "@kevin_kunzmann"
institute: "MRC-BSU, University of Cambridge"
date:      "2019/09/30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

# Who am I?

* Mathematician/Statistician with keen interest in 'Data Science' and machine (deep) learning

* Worked several years as analyst in two multi-centric studies: 4C and CENTER-TBI

* Convinced that open science/source is the future

* Sometimes frustrated by the adoption of modern *tooling* esp. in academia





---
class: inverse, middle
# Outline

1. Where are we headed? 

2. Data, Data, Data!

2. Literate Programming

3. Workflow Management

4. Version Control and Collaboration

5. Dependency Management

6. Demo





---
class: center, middle
# https://bit.ly/2jT4BYM

<iframe 
    src="https://hackmd.io/DUTDnbl7RZmGZhQVOski0A?view" 
    width="1000px" 
    height="450px">
</iframe> 





---
# Data research as software development

- modern empricial research is *highly data-driven*

- make sure that data infrastructure is adequate

- multiple stake-holders (analysts, non-tech experts)

- anything beyond absolute basics: *complex analysis scripts*

- agile development (changing requirements/hypotheses)

- quality control - reproducibility? 

<br>

> *Data research is essentially about writing a piece of software that
turns data into a manuscript $\leadsto$ adopt lessons learnt from software 
develoment!*



---
# 'Reproducible Research' + 'Open Science'

[TURING WAY]




---
class: inverse, middle, center
# 'Data is the new oil'



---
# Data = new oil is the perfect methaphor:

- need large enough storage facility (data base = tanks)

- storage must be accessible (data access = pipelines)

- infrastructure must be maintained continuously or becomes 'sticky/stale'

- security!

- on the first look, all data looks the same but there are different kinds of
data (and oil!) that cannot be mixed easily (longitudinal, cross sectional)

> Data = new oil is quite a good methaphor since it indicates 
practical storage/processing problems as well



---
# Data management considerations

- Have a plan and involve domain experts, early!

- Make sure data bases are flexible enough to adapt to changing requirements 

- Think about storage of intermediate data products

- Make sure everything is accessible in a systematic way (APIs?)

- Ensure continuous support maintainance over the entire project timeline

- Do not underestimate the effort of data curation!

- Beware that compex data bases evolve over time, how do you handle versioning/versioned access of your data?





---
class: inverse, middle
# 'Literate Programming'

> *"I believe that the time is ripe for significantly better documentation of 
programs, and that we can best achieve this by considering programs to be works
of literature. Hence, my title: 'Literate Programming'."* Donald Knuth, 1984





---
# Literate Programming

- script file: code + comments, ugly

- 'literate program': present code, output (graphs, tables), and explanatory text
as equals

- prime example 'notebooks': interactive cell-style programming

- can be exported to .pdf/.html/etc.


DEMO


---
# Notebooks

.pull-left[

Pros

1. can be exported to .pdf/.html/etc.

1. interactive coding style 

1. Edit Markdown source and refresh browser to see updated slides;
]

.pull-right[

Cons

1. usually plain notebook files in cryptic format (need custom software to display)

1. interactivity can lead to order-of-execution errors

1. limited formatting flexibility
]





---
# One step further: markdown + pandoc

<iframe 
    src="https://markdown-it.github.io/"
    width="1000px"
    height="500px">
</iframe>




---
# One step further: markdown + pandoc

- this presentation is authored in a markdown flavor!

- easy to use, source document is readable even wirth just a text editor

- minimal subset of formatting options can be converted to richer formats via
pandoc (+ templates)

- allows anthing from fancy slides to polished .pdf reports!




---
# One step further: markdown + pandoc

- pandoc: convert markdown to any output format (.html, .pdf (LaTeX), .docx)

- only need a tool to capture code expressions in markdown file and write
outputs back, then used pandoc to produce final document

- knitr for R, pweave for Python

- knitr also supports Python (and even R/Python mixed sessions!)



---
# Example: pdf report with knitr + Python

[live demo in binder]






---
# Workflow automation

- complex analyses often require multiple outputs (plots, tables, reports)

- often intermediate files, data sets used by multiple plots etc

- interdependent steps, need to maintain right order

- difficult to update everything if something changes early on

- welcome to workflow hell



---
# Workflow automation

- bear in mind: analysis = program to go from data to outputs

- should be possible to just specify what you want and have the program figure out what needs to be done (exploiting shared intermediate output)

- caching is super helpful during **development** not so much for reproducibility

- simple bash script (maybe with if clauses for checking for files) may be sufficient in simple cases

- for anything more serious, use a dedicated workflow engine





---
# snakemake

- python tool inspired by GNU make

- represents workflows as DAGs, automatically takes care of running jobs in parallel, caching etc

- can be used with slurm scheduler to exploit parallelism on a slurm cluster

- python-like syntax, easy to learn




---
# Version Control (git)

- time machine for entire projects

- distributed (no central server)

- robust (designed for wortk on Linux kernel, now also used by Microsoft!)

- **the** standard VCS .



---
# Code hosting 

- git is a command line tool

- GitHub.com/GitLab.com are code hosting platforms that make it more productive

- 





---
# GitHub.com example repo

[demo, show features]



---
# Dependency management

- source code is fine versioned

- workflow is automated

- what could go wrong?

- lots of software dependencies (R/Python/tensorflow/snakemake/LaTeX/...)

- all changing quickly - how can we make sure that the exact computing environment can be replicated in the future?

- package managers not sufficient since cross-language/system level dependencies not captured (openssl version e.g.)




---
# Solution: Containerization

- better: isolate everything but the bare operating system (linux!)

- think: lightweight virtual machine 
  - no 'hard drive'
  - no hardware virtualization (use host system) - fast!
  - images are single files, easily sharable!
  
- those must be complicated...



---
# Singularity container file

[example]
  
  
  
---
# Singularity vs. Docker

- ...



---
# Live demo on our HPC

...




---
# Summary
